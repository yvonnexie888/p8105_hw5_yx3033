p8105_hw5_yx3033
================

``` r
library(tidyverse)
library(rvest)
```

## Problem 1

``` r
birthday = function(n){
  birthdays = sample(1:365, size = n, replace = TRUE)
  any(duplicated(birthdays))
}
```

``` r
sim_groupsize_df = 
  expand_grid(
    group_size = 2:50,
    iter = 1:10000
  )
```

``` r
sim_groupsize_df  = 
  sim_groupsize_df |> 
  mutate(
    matched = map_lgl(group_size, birthday)
  )
```

``` r
#summarize probabilities

prob_df = 
  sim_groupsize_df |> 
  group_by(group_size) |> 
  summarise(probability = mean(matched))
```

``` r
ggplot(prob_df,
       aes(x = group_size, y = probability)
) +
  geom_point()+
  geom_smooth(se = FALSE)+
  labs(
    title = "Probability of Shared Birthday vs Group Size",
    x = "Group Size",
    y = "Probability"
  )
```

    ## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'

![](p8105_hw5_yx3033_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

The probability of two people share a birthday starts from 0 and
increases nonlinearly with group size. It reaches 50% when there are
around 23 people. As the group size goes beyond 40, the probability
grows start to slow down as the line gets flatter.

## Problem 2

``` r
onet_sim = function(mu, n=30, sigma = 5){
  x = rnorm(n, mean = mu, sd = sigma)
  
  test = t.test(x, mu = 0)
  
  tidy_test = broom::tidy(test)
  
  tibble(
    mu_hat = tidy_test$estimate,
    p_value = tidy_test$p.value
  )
}
```

``` r
onet_sim_df = expand_grid(
  mu_true = 0:6,
  iter = 1:5000
) |> 
  mutate(
    onet_results = map(mu_true, onet_sim)
  ) |> 
  unnest(onet_results)
```

``` r
onet_summary = 
  onet_sim_df |> 
  group_by(mu_true) |> 
  summarize(
    power = mean(p_value<0.05),
    mean_mu_hat_all = mean(mu_hat),
    mean_mu_hat_rejected = mean(mu_hat[p_value<0.05])
  )
```

``` r
ggplot(onet_summary,
       aes(y=power, x=mu_true))+
  geom_smooth(se = FALSE)+
  geom_point()+
  labs(
    title = "Power vs True Mean",
    x = "True Mean",
    y = "Power of Rejecting False H0"
  )+
  theme_minimal()
```

    ## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'

![](p8105_hw5_yx3033_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

The plot shows that the power increases with effect size. As the true
mean increases, it becomes eaiser to detect

``` r
ggplot(onet_summary,
       aes(x = mu_true))+
  geom_line(aes(y=mean_mu_hat_all, color = "All Samples"))+
  geom_line(aes(y= mean_mu_hat_rejected, color="Rejected Samples"))+
  labs(
    title = "Mean Estimate vs True Mean (All Sample & Rejected Samples",
    x = "True Mean",
    y = "Average of Estimate"
  )+
  theme_minimal()
```

![](p8105_hw5_yx3033_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->

The average of mean across all samples are similar to the true mean. But
the average mean among only rejected samples (significant samples) is
biased upward because significant results tend to have larger observed
value. This is a form of selection bias.

## Problem 3

The commit history for problem 3 is under hw5_prob3.rmd file.

``` r
homicide_raw = read_csv("data_hw5/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
summary(homicide_raw)
```

    ##      uid            reported_date       victim_last        victim_first      
    ##  Length:52179       Min.   : 20070101   Length:52179       Length:52179      
    ##  Class :character   1st Qu.: 20100318   Class :character   Class :character  
    ##  Mode  :character   Median : 20121216   Mode  :character   Mode  :character  
    ##                     Mean   : 20130899                                        
    ##                     3rd Qu.: 20150911                                        
    ##                     Max.   :201511105                                        
    ##                                                                              
    ##  victim_race         victim_age         victim_sex            city          
    ##  Length:52179       Length:52179       Length:52179       Length:52179      
    ##  Class :character   Class :character   Class :character   Class :character  
    ##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  
    ##                                                                             
    ##                                                                             
    ##                                                                             
    ##                                                                             
    ##     state                lat             lon          disposition       
    ##  Length:52179       Min.   :25.73   Min.   :-122.51   Length:52179      
    ##  Class :character   1st Qu.:33.77   1st Qu.: -96.00   Class :character  
    ##  Mode  :character   Median :38.52   Median : -87.71   Mode  :character  
    ##                     Mean   :37.03   Mean   : -91.47                     
    ##                     3rd Qu.:40.03   3rd Qu.: -81.76                     
    ##                     Max.   :45.05   Max.   : -71.01                     
    ##                     NA's   :60      NA's   :60

Comments on the raw data:  
The raw data contains 12 columns and 52,179 entries. Column variables
are uid, reported_data, victim_last (all caped), victim_first (all
caped), victim_race, victim_age, victim_sex, city, state, lat, lon, and
disposition.

Create a `city_state` variable.

``` r
homicide_df = 
  homicide_raw |>
  mutate(
    city_state = paste(city, state, sep = ", ")
  )
```

Summarize within cities to obtain the total number of homicides and the
number of unsolved homicides (those for which the disposition is “Closed
without arrest” or “Open/No arrest”).

``` r
homicide_summary  = 
  homicide_df |> 
  group_by(city_state) |> 
  summarise(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest","Open/No arrest"))
  )
```

For the city of Baltimore, MD, use the `prop.test` function to estimate
the proportion of homicides that are unsolved; save the output of
prop.test as an R object, apply the broom::tidy to this object and pull
the estimated proportion and confidence intervals from the resulting
tidy dataframe.

``` r
baltimore_summary = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |> 
  summarise(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest","Open/No arrest"))
  )
```

``` r
baltimore_prop = 
  prop.test(
    x = baltimore_summary$unsolved,
    n = baltimore_summary$total
  )
baltimore_prop_tidy  = 
  broom::tidy(baltimore_prop) |> 
  select(estimate, conf.low, conf.high)
baltimore_prop_tidy
```

    ## # A tibble: 1 × 3
    ##   estimate conf.low conf.high
    ##      <dbl>    <dbl>     <dbl>
    ## 1    0.646    0.628     0.663

run prop.test on each cities in the dataset.

``` r
homicide_results=
  homicide_summary |> 
  mutate(
    prop_test = map2(
      unsolved, total, ~ prop.test(x= .x, n= .y)
    ),
    tidy_result = map(prop_test, broom::tidy)
  ) |> 
  unnest(tidy_result) |> 
  select(city_state, estimate, conf.low, conf.high)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `prop_test = map2(unsolved, total, ~prop.test(x = .x, n = .y))`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
homicide_results
```

    ## # A tibble: 51 × 4
    ##    city_state      estimate conf.low conf.high
    ##    <chr>              <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque, NM    0.386    0.337     0.438
    ##  2 Atlanta, GA        0.383    0.353     0.415
    ##  3 Baltimore, MD      0.646    0.628     0.663
    ##  4 Baton Rouge, LA    0.462    0.414     0.511
    ##  5 Birmingham, AL     0.434    0.399     0.469
    ##  6 Boston, MA         0.505    0.465     0.545
    ##  7 Buffalo, NY        0.612    0.569     0.654
    ##  8 Charlotte, NC      0.300    0.266     0.336
    ##  9 Chicago, IL        0.736    0.724     0.747
    ## 10 Cincinnati, OH     0.445    0.408     0.483
    ## # ℹ 41 more rows

Create a plot that shows the estimates and CIs for each city.

``` r
homicide_results = 
 homicide_results |> 
  arrange(estimate)

homicide_plot = 
  homicide_results |> 
  mutate(city_state = reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, 
             y = estimate))+
  geom_point()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high))+
  coord_flip() + 
  labs(
    title = "Estimated Proportion of Unsolved Homicides by City",
    x = "City, State",
    y = "Proportion Unsolved (95% CI)"
  ) +
  theme_minimal()

homicide_plot
```

![](p8105_hw5_yx3033_files/figure-gfm/unnamed-chunk-18-1.png)<!-- -->
