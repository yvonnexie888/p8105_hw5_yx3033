---
title: "p8105_hw5_yx3033"
output: github_document
---

```{r, message=FALSE}
library(tidyverse)
library(rvest)
```

## Problem 1

```{r}
birthday = function(n){
  birthdays = sample(1:365, size = n, replace = TRUE)
  any(duplicated(birthdays))
}
```

```{r}
sim_groupsize_df = 
  expand_grid(
    group_size = 2:50,
    iter = 1:10000
  )
```

```{r}
sim_groupsize_df  = 
  sim_groupsize_df |> 
  mutate(
    matched = map_lgl(group_size, birthday)
  )
```

```{r}
#summarize probabilities

prob_df = 
  sim_groupsize_df |> 
  group_by(group_size) |> 
  summarise(probability = mean(matched))
```

```{r}
ggplot(prob_df,
       aes(x = group_size, y = probability)
) +
  geom_point()+
  geom_smooth(se = FALSE)+
  labs(
    title = "Probability of Shared Birthday vs Group Size",
    x = "Group Size",
    y = "Probability"
  )
```

The probability of two people share a birthday starts from 0 and increases nonlinearly with group size. It reaches 50% when there are around 23 people. As the group size goes beyond 40, the probability grows start to slow down as the line gets flatter. 


## Problem 2

```{r}
onet_sim = function(mu, n=30, sigma = 5){
  x = rnorm(n, mean = mu, sd = sigma)
  
  test = t.test(x, mu = 0)
  
  tidy_test = broom::tidy(test)
  
  tibble(
    mu_hat = tidy_test$estimate,
    p_value = tidy_test$p.value
  )
}
```


```{r}
onet_sim_df = expand_grid(
  mu_true = 0:6,
  iter = 1:5000
) |> 
  mutate(
    onet_results = map(mu_true, onet_sim)
  ) |> 
  unnest(onet_results)
```

```{r}
onet_summary = 
  onet_sim_df |> 
  group_by(mu_true) |> 
  summarize(
    power = mean(p_value<0.05),
    mean_mu_hat_all = mean(mu_hat),
    mean_mu_hat_rejected = mean(mu_hat[p_value<0.05])
  )
```

```{r}
ggplot(onet_summary,
       aes(y=power, x=mu_true))+
  geom_smooth(se = FALSE)+
  geom_point()+
  labs(
    title = "Power vs True Mean",
    x = "True Mean",
    y = "Power of Rejecting False H0"
  )+
  theme_minimal()
```

The plot shows that the power increases with effect size. As the true mean increases, it becomes eaiser to detect


```{r}
ggplot(onet_summary,
       aes(x = mu_true))+
  geom_line(aes(y=mean_mu_hat_all, color = "All Samples"))+
  geom_line(aes(y= mean_mu_hat_rejected, color="Rejected Samples"))+
  labs(
    title = "Mean Estimate vs True Mean (All Sample & Rejected Samples",
    x = "True Mean",
    y = "Average of Estimate"
  )+
  theme_minimal()
```

The average of mean across all samples are similar to the true mean. But the average mean among only rejected samples (significant samples) is biased upward because significant results tend to have larger observed value. This is a form of selection bias.


## Problem 3
The commit history for problem 3 is under hw5_prob3.rmd file. 

```{r}
homicide_raw = read_csv("data_hw5/homicide-data.csv")
summary(homicide_raw)
```

Comments on the raw data:  
The raw data contains 12 columns and 52,179 entries. Column variables are uid, reported_data, victim_last (all caped), victim_first (all caped), victim_race, victim_age, victim_sex, city, state, lat, lon, and disposition.  


Create a `city_state` variable.

```{r}
homicide_df = 
  homicide_raw |>
  mutate(
    city_state = paste(city, state, sep = ", ")
  )

```

Summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is "Closed without arrest" or "Open/No arrest").
```{r}
homicide_summary  = 
  homicide_df |> 
  group_by(city_state) |> 
  summarise(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest","Open/No arrest"))
  )
```

For the city of Baltimore, MD, use the `prop.test` function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe. 

```{r}
baltimore_summary = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |> 
  summarise(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest","Open/No arrest"))
  )
```
```{r}
baltimore_prop = 
  prop.test(
    x = baltimore_summary$unsolved,
    n = baltimore_summary$total
  )
baltimore_prop_tidy  = 
  broom::tidy(baltimore_prop) |> 
  select(estimate, conf.low, conf.high)
baltimore_prop_tidy
```

run prop.test on each cities in the dataset.
```{r}
homicide_results=
  homicide_summary |> 
  mutate(
    prop_test = map2(
      unsolved, total, ~ prop.test(x= .x, n= .y)
    ),
    tidy_result = map(prop_test, broom::tidy)
  ) |> 
  unnest(tidy_result) |> 
  select(city_state, estimate, conf.low, conf.high)

homicide_results
```


Create a plot that shows the estimates and CIs for each city. 

```{r, fig.width=10, fig.height=8}
homicide_results = 
 homicide_results |> 
  arrange(estimate)

homicide_plot = 
  homicide_results |> 
  mutate(city_state = reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, 
             y = estimate))+
  geom_point()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high))+
  coord_flip() + 
  labs(
    title = "Estimated Proportion of Unsolved Homicides by City",
    x = "City, State",
    y = "Proportion Unsolved (95% CI)"
  ) +
  theme_minimal()

homicide_plot
```










